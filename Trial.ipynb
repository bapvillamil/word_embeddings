{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1ca463b0e70>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as pltimport \n",
    "import numpy as np\n",
    "import random\n",
    "import nltk\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "import transformers\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# from ignite.metrics import Accuracy, Precision, Recall, Fbeta\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>when modi promised â€œminimum government maxim...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>talk all the nonsense and continue all the dra...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what did just say vote for modi  welcome bjp t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>asking his supporters prefix chowkidar their n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>answer who among these the most powerful world...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>there are two reasons for atmosphere hatred cr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>modi has wiped out the small micro industries ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>bjp struggles find candidates west bengal graf...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>modis opposition trying defame him they not wa...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>phir modi sarkar get lost yaar more chowkidar</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            clean_text  category\n",
       "0    when modi promised â€œminimum government maxim...        -1\n",
       "1    talk all the nonsense and continue all the dra...         0\n",
       "2    what did just say vote for modi  welcome bjp t...         1\n",
       "3    asking his supporters prefix chowkidar their n...         1\n",
       "4    answer who among these the most powerful world...         1\n",
       "..                                                 ...       ...\n",
       "995  there are two reasons for atmosphere hatred cr...         0\n",
       "996  modi has wiped out the small micro industries ...        -1\n",
       "997  bjp struggles find candidates west bengal graf...        -1\n",
       "998  modis opposition trying defame him they not wa...        -1\n",
       "999      phir modi sarkar get lost yaar more chowkidar         1\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Twitter_Dataset.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1    442\n",
       " 0    333\n",
       "-1    225\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_transformation(text):\n",
    "    text = re.sub('[^a-zA-Z]',' ',str(text))\n",
    "    text = text.lower()\n",
    "    text = \" \".join([word for word in text.split() if word not in stopwords.words('english')])\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also lets encode 'sentiment' column. 1 for positive and 0 for negative sentiment\n",
    "df['category'] = df['category'].map({-1:2, 1:1, 0:0}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.iloc[:, :-1].values\n",
    "y = df.iloc[:, -1:].values\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size = 0.20, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  ['talk all the nonsense and continue all the drama will vote for modi ']\n",
      "Processed:  talk nonsense continue drama vote modi\n"
     ]
    }
   ],
   "source": [
    "print('Original: ', x[1])\n",
    "print('Processed: ', text_transformation(x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():       \n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>category</th>\n",
       "      <th>processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>when modi promised â€œminimum government maxim...</td>\n",
       "      <td>2</td>\n",
       "      <td>modi promised minimum government maximum gover...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>talk all the nonsense and continue all the dra...</td>\n",
       "      <td>0</td>\n",
       "      <td>talk nonsense continue drama vote modi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what did just say vote for modi  welcome bjp t...</td>\n",
       "      <td>1</td>\n",
       "      <td>say vote modi welcome bjp told rahul main camp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>asking his supporters prefix chowkidar their n...</td>\n",
       "      <td>1</td>\n",
       "      <td>asking supporters prefix chowkidar names modi ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>answer who among these the most powerful world...</td>\n",
       "      <td>1</td>\n",
       "      <td>answer among powerful world leader today trump...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>there are two reasons for atmosphere hatred cr...</td>\n",
       "      <td>0</td>\n",
       "      <td>two reasons atmosphere hatred created modi rul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>modi has wiped out the small micro industries ...</td>\n",
       "      <td>2</td>\n",
       "      <td>modi wiped small micro industries india demoni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>bjp struggles find candidates west bengal graf...</td>\n",
       "      <td>2</td>\n",
       "      <td>bjp struggles find candidates west bengal graf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>modis opposition trying defame him they not wa...</td>\n",
       "      <td>2</td>\n",
       "      <td>modis opposition trying defame want succeed in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>phir modi sarkar get lost yaar more chowkidar</td>\n",
       "      <td>1</td>\n",
       "      <td>phir modi sarkar get lost yaar chowkidar</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            clean_text  category  \\\n",
       "0    when modi promised â€œminimum government maxim...         2   \n",
       "1    talk all the nonsense and continue all the dra...         0   \n",
       "2    what did just say vote for modi  welcome bjp t...         1   \n",
       "3    asking his supporters prefix chowkidar their n...         1   \n",
       "4    answer who among these the most powerful world...         1   \n",
       "..                                                 ...       ...   \n",
       "995  there are two reasons for atmosphere hatred cr...         0   \n",
       "996  modi has wiped out the small micro industries ...         2   \n",
       "997  bjp struggles find candidates west bengal graf...         2   \n",
       "998  modis opposition trying defame him they not wa...         2   \n",
       "999      phir modi sarkar get lost yaar more chowkidar         1   \n",
       "\n",
       "                                             processed  \n",
       "0    modi promised minimum government maximum gover...  \n",
       "1               talk nonsense continue drama vote modi  \n",
       "2    say vote modi welcome bjp told rahul main camp...  \n",
       "3    asking supporters prefix chowkidar names modi ...  \n",
       "4    answer among powerful world leader today trump...  \n",
       "..                                                 ...  \n",
       "995  two reasons atmosphere hatred created modi rul...  \n",
       "996  modi wiped small micro industries india demoni...  \n",
       "997  bjp struggles find candidates west bengal graf...  \n",
       "998  modis opposition trying defame want succeed in...  \n",
       "999           phir modi sarkar get lost yaar chowkidar  \n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets apply above function to every tweet in df\n",
    "df['processed'] = df['clean_text'].apply(text_transformation)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_TRAINED_MODEL_NAME = 'distilbert-base-cased'\n",
    "\n",
    "# Lets load pre-trained Distill BertTokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: talk nonsense continue drama vote modi\n",
      "Tokens: ['talk', 'nonsense', 'continue', 'drama', 'vote', 'm', '##od', '##i']\n",
      "Token IDs: [2037, 17466, 2760, 3362, 2992, 182, 5412, 1182]\n"
     ]
    }
   ],
   "source": [
    "# Lets use below text to understand tokenization process\n",
    "# First I am processing our review using above defined function\n",
    "sample_text = text_transformation(df['processed'][1])\n",
    "\n",
    "# Lets apply our BertTokenizer on sample text\n",
    "tokens = tokenizer.tokenize(sample_text)    # this will convert sentence to list of words\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens) # this will convert list of words to list of numbers based on tokenizer\n",
    "\n",
    "print(f'Sentence: {sample_text}')\n",
    "print(f'Tokens: {tokens}')\n",
    "print(f'Token IDs: {token_ids}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  101,  2037, 17466,  2760,  3362,  2992,   182,  5412,  1182,   102,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding = tokenizer.encode_plus(\n",
    "  sample_text,\n",
    "  max_length=32,  # Here for experiment I gave 32 as max_length\n",
    "  truncation = True,  # Truncate to a maximum length specified with argument max_length\n",
    "  add_special_tokens=True, # Add '[CLS]', [PAD] and '[SEP]'\n",
    "  return_token_type_ids=False,  # since our use case deals with only one sentence as opposed to use case which use 2 sentences in single training example(for ex: Question-anwering) we can have it as false\n",
    "  padding='max_length',   # pad to longest sequence as defined by max_length\n",
    "  return_attention_mask=True,  # Returns attention mask. Attention mask indicated to the model which tokens should be attended to, and which should not.\n",
    "  return_tensors='pt',  # Return PyTorch tensors\n",
    ")\n",
    "\n",
    "print(len(encoding['input_ids'][0]))\n",
    "encoding['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Attention mask also has same length. Zero's in output if any says those corresponds to padding\n",
    "print(len(encoding['attention_mask'][0]))\n",
    "encoding['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 512\n",
    "\n",
    "class IMDBDataset(Dataset):\n",
    "  def __init__(self, reviews, targets, tokenizer, max_len):\n",
    "    self.reviews = reviews\n",
    "    self.targets = targets\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_len = max_len\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.reviews)\n",
    "\n",
    "  # __getitem__ helps us to get a review out of all reviews\n",
    "  def __getitem__(self, item):\n",
    "    review = str(self.reviews[item])\n",
    "    target = self.targets[item]\n",
    "\n",
    "    encoding = self.tokenizer.encode_plus(\n",
    "      review,\n",
    "      add_special_tokens=True,\n",
    "      max_length=self.max_len,\n",
    "      truncation = True,\n",
    "      return_token_type_ids=False,\n",
    "      padding='max_length',\n",
    "      return_attention_mask=True,\n",
    "      return_tensors='pt',\n",
    "    )\n",
    "\n",
    "    return {\n",
    "      'review_text': review,\n",
    "      'input_ids': encoding['input_ids'].flatten(),         # flatten() flattens a continguous range of dims in a tensor\n",
    "      'attention_mask': encoding['attention_mask'].flatten(),\n",
    "      'targets': torch.tensor(target, dtype=torch.long)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(700, 3) (150, 3) (150, 3)\n"
     ]
    }
   ],
   "source": [
    "# Lets have 70% for training, 15% for validation and 15% for testing\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(df[['processed', 'clean_text']], df['category'], \n",
    "                                                    test_size=0.30, random_state = 0)\n",
    "df_train = pd.concat([pd.DataFrame({'clean_text': X_train['processed'].values,'review_old':X_train['clean_text'].values}),pd.DataFrame({'category': y_train.values})], axis = 1)\n",
    "df_valid = pd.concat([pd.DataFrame({'clean_text': X_valid['processed'].values,'review_old':X_valid['clean_text'].values}),pd.DataFrame({'category': y_valid.values})], axis = 1)\n",
    "\n",
    "\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(df_valid[['clean_text','review_old']], df_valid['category'],\n",
    "                                                    test_size=0.5, random_state = 0)\n",
    "\n",
    "df_valid = pd.concat([pd.DataFrame({'clean_text': X_valid['clean_text'].values,'review_old':X_valid['review_old'].values}),pd.DataFrame({'category': y_valid.values})], axis = 1)\n",
    "df_test = pd.concat([pd.DataFrame({'clean_text': X_test['clean_text'].values,'review_old':X_test['review_old'].values}),pd.DataFrame({'category': y_test.values})], axis = 1)\n",
    "\n",
    "print(df_train.shape, df_valid.shape, df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "  ## pass in entire data set here\n",
    "  ds = IMDBDataset(\n",
    "    reviews=df.clean_text.to_numpy(),\n",
    "    targets=df.category.to_numpy(),\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=max_len\n",
    "  )\n",
    "  # this returns dataloaders with what ever batch size we want\n",
    "  return DataLoader(\n",
    "    ds,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=4                 \n",
    "    # tells data loader how many sub-processes to use for data loading. No hard and fast rule. Have to experiment on how many num_workers giving better speed up\n",
    "  )\n",
    "\n",
    "\n",
    "batch_size = 16      # Bert recommendation\n",
    "\n",
    "train_data_loader = create_data_loader(df_train, tokenizer, max_len, batch_size)\n",
    "valid_data_loader = create_data_loader(df_valid, tokenizer, max_len, batch_size)\n",
    "test_data_loader = create_data_loader(df_test, tokenizer, max_len, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Lets build classifier for our reviews now. Below n_classes would be 2 in our case since we are classifying review as either positive or negative.\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained(PRE_TRAINED_MODEL_NAME, num_labels = 3)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\JDEGUZMAN\\Desktop\\Python for Data Scientist\\revalida-nlp\\env\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr = 5e-5)\n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "  optimizer,\n",
    "  num_warmup_steps=0,\n",
    "  num_training_steps=total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets write a function to train our model on one epoch\n",
    "\n",
    "def train_epoch(model, data_loader, optimizer, device, scheduler, n_examples):\n",
    "\n",
    "  model = model.train()    # tells your model that we are training\n",
    "  losses = []\n",
    "  correct_predictions = 0\n",
    "\n",
    "  for d in data_loader:\n",
    "    input_ids = d[\"input_ids\"].to(device)\n",
    "    attention_mask = d[\"attention_mask\"].to(device)\n",
    "    targets = d[\"targets\"].to(device)\n",
    "\n",
    "    loss, logits = model(\n",
    "      input_ids=input_ids,\n",
    "      attention_mask=attention_mask,\n",
    "      labels = targets\n",
    "    )\n",
    "    \n",
    "    #logits = classification scores befroe softmax\n",
    "    #loss = classification loss\n",
    "    \n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = targets.to('cpu').numpy()\n",
    "\n",
    "    preds = np.argmax(logits, axis=1).flatten()   #returns indices of maximum logit\n",
    "    targ = label_ids.flatten()\n",
    "\n",
    "    correct_predictions += np.sum(preds == targ)\n",
    "\n",
    "    losses.append(loss.item())\n",
    "    loss.backward()   # performs backpropagation(computes derivates of loss w.r.t to parameters)\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  #clipping gradients so they dont explode\n",
    "    optimizer.step()       #After gradients are computed by loss.backward() this makes the optimizer iterate over all parameters it is supposed to update and use internally #stored grad to update their values\n",
    "    scheduler.step()    # this will make sure learning rate changes. If we dont provide this learning rate stays at initial value\n",
    "    optimizer.zero_grad()     # clears old gradients from last step\n",
    "\n",
    "  return correct_predictions / n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets write a function to validate our model on one epoch\n",
    "\n",
    "def eval_model(model, data_loader, device, n_examples):\n",
    "  \n",
    "  model = model.eval()   # tells model we are in validation mode\n",
    "  losses = []\n",
    "  correct_predictions = 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for d in data_loader:\n",
    "      input_ids = d[\"input_ids\"].to(device)\n",
    "      attention_mask = d[\"attention_mask\"].to(device)\n",
    "      targets = d[\"targets\"].to(device)\n",
    "      loss, logits = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        labels = targets\n",
    "      )\n",
    "\n",
    "\n",
    "      logits = logits.detach().cpu().numpy()\n",
    "      label_ids = targets.to('cpu').numpy()\n",
    "\n",
    "      preds = np.argmax(logits, axis=1).flatten()\n",
    "      targ = label_ids.flatten()\n",
    "\n",
    "      correct_predictions += np.sum(preds == targ)\n",
    "      losses.append(loss.item())\n",
    "\n",
    "  return correct_predictions / n_examples, np.mean(losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "# standard block\n",
    "# used accuracy as metric here\n",
    "\n",
    "best_acc = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "  print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "  print('-' * 10)\n",
    "\n",
    "  train_acc, train_loss = train_epoch(model, train_data_loader, optimizer, device, scheduler, len(df_train))\n",
    "\n",
    "  print(f'Train loss {train_loss} Accuracy {train_acc}')\n",
    "\n",
    "  val_acc, val_loss = eval_model(model, valid_data_loader, device, len(df_valid))\n",
    "\n",
    "  print(f'Val   loss {val_loss} Accuracy {val_acc}')\n",
    "  print()\n",
    "\n",
    "  if val_acc > best_acc:\n",
    "    torch.save(model.state_dict(), 'best_model_state_a5.bin')\n",
    "    best_acc = val_acc\n",
    "\n",
    "# We are storing state of best model indicated by highest validation accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dafc7c855939df8ce795faef88429f3deffc2b9f888644885597ad2b84a1d158"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
